# SOLA Architecture Specification

## 1. Core Philosophy: Vertical Slice Architecture
We reject the traditional "Layered Architecture" (controllers/services/models) in favor of **Vertical Slices**.
Each feature in a SOLA project is a self-contained module that handles its own logic, data, and API exposure.

### Why?
- **Cognitive Load:** Developers focus on ONE feature at a time.
- **Scalability:** Features can be easily extracted into separate microservices.
- **Safety:** Changes in one feature (e.g., `document_parser`) won't break another (e.g., `audit_logger`).

---

## 2. Directory Structure via `sola init`
A standard SOLA project MUST follow this structure:

```text
my-ai-service/
├── app/
│   ├── main.py              # App entrypoint (registers routers)
│   ├── config.py            # Global settings (Pydantic Settings)
│   ├── features/            # VERTICAL SLICES LIVE HERE
│   │   ├── __init__.py
│   │   ├── health/          # Standard Health Check Slice
│   │   │   ├── router.py
│   │   │   └── schemas.py
│   │   └── [feature_name]/  # Generated by `sola build`
│   │       ├── router.py    # FastAPI Endpoints
│   │       ├── service.py   # Business Logic (LLM calls)
│   │       └── schemas.py   # Pydantic V2 Models
│   └── shared/              # Truly shared utilities (Logging, Auth)
├── tests/
│   ├── unit/
│   └── evals/               # DeepEval AI Quality Gates
├── .github/workflows/       # CI/CD
├── Dockerfile               # Production Container
├── Makefile                 # Automation
├── pyproject.toml           # Config
└── README.md
```

## 3. Technology Enforcements
- **Package Manager:** `uv`
- **Web Framework:** `FastAPI` (Async Only)
- **LLM Interface:** `AsyncOpenAI` (Native Structured Outputs)
- **Validation:** `pydantic` V2 (Strict Mode)
- **Evaluation:** `deepeval` (Metric-driven testing)

---

## 4. The "AI Architect" Standards (`sola build`)
When generating code, the AI/LLM must adhere to:
1.  **Type Safety:** No `Dict` or `Any`. Use Pydantic models for EVERYTHING.
2.  **Async First:** All I/O (Database, OpenAI) must be `await`.
3.  **Structured Output:** LLM responses must be parsed into Objects, not plain strings.
4.  **Error Handling:** Graceful degradation if LLM fails or hallucinates.
